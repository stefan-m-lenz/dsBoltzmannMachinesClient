% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/main.R
\name{ds.monitored_fitdbm}
\alias{ds.monitored_fitdbm}
\title{Fits a (multimodal) DBM model}
\usage{
ds.monitored_fitdbm(datasources, newobj = "dbm", data = "D",
  monitoring = "logproblowerbound", monitoringdata = data,
  monitoringpretraining = "reconstructionerror",
  monitoringdatapretraining = monitoringdata, nhiddens = NULL,
  epochs = NULL, nparticles = NULL, learningrate = NULL,
  learningrates = NULL, learningratepretraining = NULL,
  epochspretraining = NULL, batchsizepretraining = NULL,
  pretraining = NULL)
}
\arguments{
\item{datasources}{A list of Opal object(s) as a handle to the server-side session}

\item{newobj}{The name for the variable, in which the trained DBM will be stored.
Defaults to \code{"dbm"}}

\item{data}{The name of the variable that holds the data on the server-side.
Defaults to \code{"D"}.}

\item{monitoring}{Name(s) for the monitoring options used for monitoring the fine-tuning.
Possible options:
\itemize{
\item \code{"logproblowerbound"}: Variational lower bound of log probability (Default)
\item \code{"exactloglikelihood"}: Exact calculation of log-likelihood.
This is only feasible for very small models.
\item \code{NULL}: No monitoring
}}

\item{monitoringdata}{A vector of names for server-side data sets that are to be used for
monitoring}

\item{monitoringpretraining}{Name for monitoring options used for monitoring the pre-training.
The options are the same as for
training an RBM (see \code{\link{ds.monitored_fitrbm}}).
By default, the reconstruction error is monitored.}

\item{monitoringdatapretraining}{A vector of names for data sets that are to be used for
monitoring the pretraining. By default, this is the same as the \code{monitoringdata}.}

\item{nhiddens}{A vector that defines the number of nodes in the hidden layers of
the DBM. The default value specifies two hidden layers with the same size
as the visible layer.}

\item{epochs}{Number of training epochs for fine-tuning, defaults to 10}

\item{nparticles}{Number of particles used for sampling during fine-tuning of the
DBM, defaults to 100}

\item{learningrate}{Learning rate for joint training of layers (= fine-tuning)
using the learning algorithm for a general Boltzmann Machine
with mean-field approximation.
The learning rate for fine tuning is by default decaying with the number of epochs,
starting with the given value for the \code{learningrate}.
By default, the learning rate decreases with the factor \eqn{11 / (10 + epoch)}.}

\item{learningrates}{a vector of learning rates for each epoch of fine-tuning}

\item{learningratepretraining}{Learning rate for pretraining,
defaults to \code{learningrate}}

\item{epochspretraining}{Number of training epochs for pretraining,
defaults to \code{epochs}}

\item{batchsizepretraining}{Batchsize for pretraining, defaults to 1}

\item{pretraining}{The arguments for layerwise pretraining
can be specified for each layer individually.
This is done via a vector of names for objects that have previously been defined
by \code{\link{ds.bm.defineLayer}} or \code{\link{ds.bm.definePartitionedLayer}}.
(For a detailed description of the possible parameters,
see the help there).
If the number of training epochs and the learning rate are not specified
explicitly for a layer, the values of \code{epochspretraining},
\code{learningratepretraining} and \code{batchsizepretraining} are used.}
}
\description{
The procedure for DBM fitting consists of two parts:
First a stack of RBMs is pretrained in a greedy layerwise manner
(see \code{\link{ds.monitored_stackrbms}}). Then the weights of all layers are jointly
trained using the general Boltzmann Machine learning procedure.
During pre-training and fine-tuning, monitoring data is collected by default.
The monitoring data is returned to the user.
The trained model is stored on the server side (see parameter \code{newobj}).
}
\details{
If the option \code{datashield.BoltzmannMachines.shareModels} is set to \code{TRUE}
by an administratorat the server side, the models themselves are returned in addition.
}
